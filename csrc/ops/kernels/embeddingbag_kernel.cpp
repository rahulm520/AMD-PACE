/*******************************************************************************
 * Modifications Copyright (c) 2024 Advanced Micro Devices, Inc. All rights
 * reserved. Notified per clause 4(b) of the license.
 * Portions of this file consist of AI-generated content
 *******************************************************************************/

#include <ATen/Parallel.h>
#include <fbgemm/FbgemmEmbedding.h>
#include <ops/kernels/embeddingbag_kernel.h>

namespace pace {

namespace kernels {

// Copy of
// aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:fbgemm_spmdm_report_error_()
// to handle the errors generated by the fbgemm kernel
template <typename IndexType, typename OffsetType>
void fbgemm_spmdm_report_error_(
    int64_t output_size,
    int index_size,
    int64_t N,
    const OffsetType* offsets,
    const IndexType* indices) {
  for (const auto m : c10::irange(output_size)) {
    for (OffsetType i = offsets[m]; i < offsets[m + 1]; ++i) {
      TORCH_CHECK(i < index_size);
      IndexType idx = indices[i];
      TORCH_CHECK(
          0 <= idx && idx < N,
          "Index ",
          i,
          " is out of bounds: ",
          idx,
          ", range 0 to ",
          N);
    }
  }
  TORCH_CHECK(
      offsets[output_size] == index_size,
      "Yout input seems to be incorrect: the last offset value should be "
      "the size of the indices tensor, but it appears not.");
}

/**
 * @brief Modified version from
 * aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:embedding_bag_nbit_impl()
 * Had to make a copy of to support the inplace outputs.
 *
 * @tparam IndexType
 * @tparam OffsetType
 * @param output Tensor where output should be written
 * @param weight Weight matrix for the embedding tables
 * @param indices Index for the embedding tables
 * @param offsets_in Offsets for the embedding tables
 * @param bit_width Number of bits used for storing embedding bag weights
 * @param num_embedding_bags The total number of embedding tables
 * @param output_size The expected output size of the embedding bag
 * @param idx Index of the current embedding table from the total number of
 * embedding tables
 */
template <typename IndexType, typename OffsetType>
void embedding_bag_nbit_impl_with_strides(
    at::Tensor& output,
    const at::Tensor& weight,
    const at::Tensor& indices,
    const at::Tensor& offsets,
    const int bit_width,
    const int num_embedding_bags,
    const int output_size,
    const int idx) {
  TORCH_CHECK(weight.scalar_type() == at::kByte);
  TORCH_CHECK(weight.dim() == 2);
  TORCH_CHECK(offsets.dim() == 1);
  auto offsets_data = offsets.data_ptr<OffsetType>();

  const auto weight_sizes = weight.sizes();
  // D is calculated according the bit width
  // if bit_width == 8: 8 bits to account for scale and bias
  // else: 2-byte fp16 scale and 2-byte zero_offset
  const int64_t D = (bit_width == 8)
      ? weight_sizes[1] - 8
      : (weight_sizes[1] - 2 * sizeof(at::Half)) * (8 / bit_width);
  const int64_t M = offsets.sizes()[0];

  int64_t batch_size = M - 1;
  TORCH_CHECK(
      (output_size == D),
      "pace::embedding_bag_nbit_impl: Expected output size is ",
      D,
      " but recieved ",
      output_size,
      " instead");
  const int output_stride = D * (num_embedding_bags + 1);

  const int64_t N = weight_sizes[0];
  const auto weight_data = weight.data_ptr<uint8_t>();
  const auto indices_data = indices.data_ptr<IndexType>();
  auto* output_data = output.data_ptr<float>() + (idx * D) + D;

  auto kernel = fbgemm::GenerateEmbeddingSpMDMWithStrides<
      uint8_t,
      IndexType,
      OffsetType,
      /*OutType=*/float,
      /*TRHEAD_LOCAL=*/true>(
      /*block_size=*/D,
      /*has_weight=*/false,
      /*normalize_by_lengths=*/false,
      /*prefetch=*/16, // NOLINT(cppcoreguidelines-avoid-magic-numbers)
      /*is_weight_positional=*/false,
      /*use_offsets=*/true,
      /*output_stride*/ output_stride,
      /*input_stride=*/-1,
      /*scale_bias_last=*/true,
      /*no_bag=*/false);

  if (bit_width == 4) {
    kernel = fbgemm::GenerateEmbeddingSpMDMNBitWithStrides<
        IndexType,
        OffsetType,
        /*OutType=*/float,
        /*TRHEAD_LOCAL=*/true>(
        /*bit_rate*/ bit_width,
        /*block_size=*/D,
        /*has_weight=*/false,
        /*normalize_by_lengths=*/false,
        /*prefetch=*/16, // NOLINT(cppcoreguidelines-avoid-magic-numbers)
        /*is_weight_positional=*/false,
        /*use_offsets=*/true,
        /*output_stride*/ output_stride,
        /*input_stride=*/-1,
        /*scale_bias_last=*/true,
        /*no_bag=*/false);
  }

  at::parallel_for(0, batch_size, 1, [&](int64_t start_idx, int64_t end_idx) {
    bool success = kernel(
        /*output_size=*/end_idx - start_idx,
        /*index_size=*/offsets_data[end_idx] - offsets_data[start_idx],
        /*data_size=*/N,
        /*input=*/weight_data,
        /*indices=*/indices_data + offsets_data[start_idx],
        /*offsets_or_lengths=*/offsets_data + start_idx,
        /*weights=*/nullptr,
        /*out=*/output_data + start_idx * output_stride);

    if (!success) {
      fbgemm_spmdm_report_error_(
          end_idx - start_idx,
          offsets_data[end_idx] - offsets_data[start_idx],
          N,
          offsets_data + start_idx,
          indices_data + offsets_data[start_idx]);
    }
  });
}

void qembedding_bag_nbit_with_stride(
    at::Tensor& output,
    const at::Tensor& weight,
    const at::Tensor& indices,
    const at::Tensor& offsets,
    const int bit_width,
    const int num_embedding_bags,
    const int output_size,
    const int idx) {
  TORCH_CHECK(
      (bit_width == 4 || bit_width == 8),
      "pace::qmerged_embedding_bag_nbit_cat operator supports 4 or 8 bits "
      "but got ",
      bit_width);
  TORCH_CHECK(
      indices.dim() == 1,
      "pace::qmerged_embedding_bag_nbit_cat operator supports 1d "
      "indices, got ",
      indices.dim());

  TORCH_CHECK(
      indices.scalar_type() == at::kInt || indices.scalar_type() == at::kLong,
      "Expected Int or Long indices, but found ",
      indices.scalar_type(),
      " instead.");
  TORCH_CHECK(
      offsets.scalar_type() == at::kInt || offsets.scalar_type() == at::kLong,
      "Expected Int or Long offsets, but found ",
      offsets.scalar_type(),
      " instead.");
  TORCH_CHECK(
      weight.is_contiguous() && indices.is_contiguous() &&
          offsets.is_contiguous(),
      "Expected weight, indices, and offsets to be contiguous.");

  TORCH_CHECK(
      (offsets[offsets.sizes()[0] - 1].item().toUInt64() == indices.sizes()[0]),
      "Last element in offsets should be equal to the number of elements in indices, but got ",
      offsets[offsets.sizes()[0] - 1].item().toUInt64(),
      " and ",
      indices.sizes()[0],
      " at index ",
      idx);

  // Using helper function to support different type combination without the
  // need to cast, which can be additional performance overhead
  if (indices.scalar_type() == at::kInt && offsets.scalar_type() == at::kInt) {
    embedding_bag_nbit_impl_with_strides<int, int>(
        output,
        weight,
        indices,
        offsets,
        bit_width,
        num_embedding_bags,
        output_size,
        idx);
  } else if (
      indices.scalar_type() == at::kInt && offsets.scalar_type() == at::kLong) {
    embedding_bag_nbit_impl_with_strides<int, int64_t>(
        output,
        weight,
        indices,
        offsets,
        bit_width,
        num_embedding_bags,
        output_size,
        idx);
  } else if (
      indices.scalar_type() == at::kLong && offsets.scalar_type() == at::kInt) {
    embedding_bag_nbit_impl_with_strides<int64_t, int>(
        output,
        weight,
        indices,
        offsets,
        bit_width,
        num_embedding_bags,
        output_size,
        idx);
  } else {
    // default case given the TORCH_CHECK above
    embedding_bag_nbit_impl_with_strides<int64_t, int64_t>(
        output,
        weight,
        indices,
        offsets,
        bit_width,
        num_embedding_bags,
        output_size,
        idx);
  }
  return;
}

} // namespace kernels

} // namespace pace
